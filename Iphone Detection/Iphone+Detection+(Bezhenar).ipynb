{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iphone or not Iphone detection\n",
    "*Bezhenaer OLga*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection:\n",
    "I honestly did not cope with the data collection, so my colleagues shared it with me. However, I carried out the primary cleaning of data from garbage (pictures downloaded from avito on the subject of \"iPhone\" that do not contain photos of iPhones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import of libraries \n",
    "from keras.preprocessing import image\n",
    "from keras.applications import resnet50, inception_v3, vgg16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lite\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/classification_models.git\n",
      "  Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-925gaong-build\n",
      "  Requirement already satisfied (use --upgrade to upgrade): image-classifiers==0.2.2 from git+https://github.com/qubvel/classification_models.git in ./anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: keras>=2.1.0 in ./anaconda3/lib/python3.6/site-packages (from image-classifiers==0.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in ./anaconda3/lib/python3.6/site-packages (from keras>=2.1.0->image-classifiers==0.2.2)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing Resnet18\n",
    "!pip install git+https://github.com/qubvel/classification_models.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing Resnet18\n",
    "from classification_models.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/st039712/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Loading the model (RESNET18) with pre-trained weights\n",
    "\n",
    "num_classes = 2 \n",
    "\n",
    "base_model = ResNet18 \n",
    "base_model = ResNet18(weights='imagenet',input_shape=(224,224,3), include_top=False) \n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output) \n",
    "predictions = Dense(512, activation='relu')(x) \n",
    "predictions = Dense(num_classes, activation='softmax')(predictions) \n",
    "model = Model(inputs=[base_model.input], outputs=predictions) \n",
    "\n",
    "for layer in model.layers: \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image transformation & data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating rule for images random transformation  (ramdom transformation)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chosing batch size - parameter for modael tunning \n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48902 images belonging to 2 classes.\n",
      "Found 2035 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting the path to images (creating data loaders)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'full dataset/train',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        'full dataset/val',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'full dataset/test',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 150x150\n",
    "#          batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AOther': 0, 'Iphones': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive - Iphones, negative - Not iphones \n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define sample sizes for further calculations \n",
    "nb_samples_train = len(train_generator.filenames)\n",
    "nb_samples_val= len(val_generator.filenames)\n",
    "nb_samples_test = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Final step of the model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step of model building (main hyperparameter for tunning)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['acc']) # \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading weights (was used to save the results of previous model training)\n",
    "model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/st039712/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "765/764 [==============================] - 688s 900ms/step - loss: 0.0410 - acc: 0.9845 - val_loss: 0.2530 - val_acc: 0.9283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ca4d23518>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_samples_train/batch_size,\n",
    "        epochs=200,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=nb_samples_val/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.hdf5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction of the file size (compressing the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Tensorflow Lite, as the way to solve problem with loading large files in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/st039712/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/st039712/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 100 variables.\n",
      "INFO:tensorflow:Converted 100 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45785876"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting keras model to tflite model\n",
    "converter = lite.TFLiteConverter.from_keras_model_file('model.hdf5')\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model \"size\" now is about 45MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement of the model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for loading and rescaling images\n",
    "def load_image(img_path):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    \n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         \n",
    "    img_tensor /= 255.\n",
    "    \n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dir='full dataset/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#walking through our validation set and get predictions for each photo\n",
    "compr_pred = []\n",
    "for subdir, dirs, files in os.walk(val_dir):\n",
    "    for file in files:\n",
    "        input_data = load_image(os.path.join(subdir, file))\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        compr_pred.append(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score for compressed model: 0.9850241076\n"
     ]
    }
   ],
   "source": [
    "#result's check \n",
    "average_precision_compr = average_precision_score(val_generator.classes, [x[1] for x in compr_pred])\n",
    "print('Average precision-recall score for compressed model: {0:0.10f}'.format(average_precision_compr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(val_generator.classes, [x[1] for x in compr_pred])\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision_compr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! conda list -e > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
